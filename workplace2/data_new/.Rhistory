install.packages(c("cli", "lubridate", "RcppEigen", "stringi", "timechange"))
install.packages("lubridate")
R.Version()
print(R.Version())
v <- R.Version()
print(v)
install.packages("installr")
library(installr)
updateR()
load_libraries <- function(){
library(dplyr)
library(ggplot2)
library(ggpubr)
library(GGally)
library(caret)
library(doParallel)
library(plotly)
}
load_libraries()
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
# Loading the necessary libraries
load_libraries()
# loading the data files
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new")
databases()  # loading the databases metaDBphyto and metaDBzoo
phyto_raw <- read.csv("phyto.smallzoo.86-19_BAL.csv", sep = ",")  # loading the raw phytoplankton data
zoo_raw <- read.csv("largezoo.86-19_BAL.csv")  # loading the raw zooplankton data
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new/temp_chem")  # loading the raw temperature and chemistry data
temp_chem <- read.csv("temp.chem.85-20_BAL.csv")
lake <- "BAL"  # Defining the name of the lake. This will be used when saving data in external files, to identify them later on
# Getting the phytoplankton data ready
ready_phyto_selec()  # setting the right data formats for the columns and adding the corresponding information from the meta database, for each entry
smallzoo <- inner_join(phyto_raw, (meta_db_zoo %>% select(id_CH, phylum, order, family, genus, species, stage, volume_um.3, guild)), by = "id_CH")  # creating a separate data frame containing the small zooplankton
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
# Loading the necessary libraries
load_libraries()
# loading the data files
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new")
databases()  # loading the databases metaDBphyto and metaDBzoo
phyto_raw <- read.csv("phyto.smallzoo.86-19_BAL.csv", sep = ",")  # loading the raw phytoplankton data
zoo_raw <- read.csv("largezoo.86-19_BAL.csv")  # loading the raw zooplankton data
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new/temp_chem")  # loading the raw temperature and chemistry data
temp_chem <- read.csv("temp.chem.85-20_BAL.csv")
lake <- "BAL"  # Defining the name of the lake. This will be used when saving data in external files, to identify them later on
# Getting the phytoplankton data ready
ready_phyto_selec()  # setting the right data formats for the columns and adding the corresponding information from the meta database, for each entry
View(smallzoo)
treat_noBiovolP(lake)  # Isolating the phyto data missing a biovolume value in the meta database
ready_phyto_clean()  # Removing entries without a metaDB-biovolume value and calculating the biovolume for the ones we keep
ready_phyto_agg()  # Aggregating phytoplankton biovolume per order and date
ready_phyto_final()  # Making the phytoplankton data ready for analysis and calculating difference and ratio between the phyto orders and the total phyto biovolume per day
# Getting the zooplankton data ready
ready_zoo_raw()  # Setting the right data formats
ready_zoo_agg()  # Aggregating the abundance of chosen orders/phyla/families per day
nrow(zoo_selec)
nrow(zoo_agg)
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
# Getting the phytoplankton data ready
ready_phyto_selec()  # setting the right data formats for the columns and adding the corresponding information from the meta database, for each entry
# Getting the zooplankton data ready
ready_zoo_raw()  # Setting the right data formats
ready_zoo_agg()  # Aggregating the abundance of chosen orders/phyla/families per day
nrow(zoo_agg)
zoo_selec <- inner_join(zoo_raw, (meta_db_zoo %>% select(id_CH, phylum, order, family, genus, species, stage, volume_um.3, guild)), by="id_CH")  # Adding the needed info columns to the data table
nrow(zoo_selec)
zoo_selec <- rbind(zoo_selec, smallzoo)  # adding the small zoo to the data frame
nrow(zoo_selec)
# Tidying up the data to make it ready for use
zoo_agg <- zoo_selec[complete.cases(zoo_selec$abundance),] # removes data points without an abundance
nrow(zoo_agg)
tot_zoo <- aggregate(zoo_agg$abundance, by = list(zoo_agg$date), sum)  # Summing all the zooplankton abundance, per day.
names(tot_zoo) <- c("date", "tot_zoo")
# Extracting the microzooplankton from the data frame
naupleii <- zoo_agg %>% filter(((order == "Calanoida" | order == "Cyclopoida") & stage == "juvenile") | stage == "nauplia")  # extracting the younger states of the copepods, called "naupleii", "nauplii" or "nauplia" depending on the literature
roti_cilio <- zoo_agg %>% filter(phylum == "Rotifera" | phylum == "Ciliophora")  # extracting the Rotifera and Ciliophora (since they are phyla, they can't be aggregated by order  -> need to be aggregated separately)
# aggregating the microzooplankton on its own, by date
microzoo <- bind_rows(naupleii, roti_cilio)  # creating the data frame with Ciliates, Rotifers and Naupleii
microzoo_taxa <- microzoo %>% distinct(id_CH)  # This creates a list of the microzooplankton, so we can then exclude them from the zoo_agg data frame
microzoo <- aggregate(list(microzoo$abundance), by = list(microzoo$date), sum)
names(microzoo) <- c("date", "microzoo")
zoo_agg <- zoo_agg %>% anti_join(microzoo_taxa, by = "id_CH")  # removing the microzooplankton from the rest of the data frame (they will be put back into the table later on)
zoo_agg <- zoo_agg %>% filter(order == "Calanoida"| order == "Cyclopoida"| family == "Daphniidae")  # only keeping the relevant orders and families
# Aggregating by order, for each date
zoo_agg <- aggregate(list(zoo_agg$abundance), by= list(zoo_agg$order, zoo_agg$date), sum)
names(zoo_agg) <- c("order", "date", "abundance")
# Adding the total zooplankton and the microzoo back
zoo_agg <- inner_join(zoo_agg, tot_zoo, by = "date")  # adding the total zoo to the table
zoo_agg <- inner_join(zoo_agg, microzoo, by = "date")  # Adding the microzooplankton back to the table (Rotifera, Ciliophora, Naupleii)
# Next 3 lines show that the orders weren't always observed --> consequence: there are NA's in the data frame
cat("Daphniidae were observed", nrow(zoo_agg %>% filter(order == "Diplostraca")), "times\n")
cat("Calanoida were observed", nrow(zoo_agg %>% filter(order == "Calanoida")), "times\n")
cat("Cyclopoida were observed", nrow(zoo_agg %>% filter(order == "Cyclopoida")), "times")
# making the data frames global
zoo_agg <<- zoo_agg
zoo_taxa <<- zoo_taxa
microzoo_taxa <<- microzoo_taxa
nrow(zoo_agg)
zoo_selec <- inner_join(zoo_raw, (meta_db_zoo %>% select(id_CH, phylum, order, family, genus, species, stage, volume_um.3, guild)), by="id_CH")  # Adding the needed info columns to the data table
nrow(zoo_selec)
zoo_selec <- bind_rows(zoo_selec, smallzoo)  # adding the small zoo to the data frame
nrow(zoo_selec)
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
# Loading the necessary libraries
load_libraries()
# loading the data files
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new")
databases()  # loading the databases metaDBphyto and metaDBzoo
phyto_raw <- read.csv("phyto.smallzoo.85-20_GRE.csv", sep = ",", encoding = "URF-8")  # loading the raw phytoplankton data
zoo_raw <-  read.csv("largezoo.85-20_GRE.csv")  # loading the raw zooplankton data
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new/temp_chem")  # loading the raw temperature and chemistry data
temp_chem <- read.csv("temp.chem.42-20_GRE.csv")
lake <- "GRE"  # Defining the name of the lake. This will be used when saving data in external files, to identify them later on
# Specific to the GRE lake: "id_CH" and "date" show problems
{
class(phyto_raw$id_CH)  #it's a chr, which means we'll have to change that
# this part isolates the id_CH that are not id's but already species names in the id_CH field
num_only <- phyto_raw
num_only$id_CH <- as.integer(num_only$id_CH)  # "NAs introduced by coercion" is normal and wanted: it marks the rows whose id_CH are not integers with NA
num_only <- num_only[complete.cases(num_only$id_CH),]  # isolates the non-integer id_CH rows
num_only$id_CH <- as.character(num_only$id_CH)  # now transforming the id_CH back to a character. This is necessary for the anti_join in the next step: the id_CH in num_only and in phyto_raw need to be the same type
char_only <- phyto_raw %>% anti_join(num_only, by="id_CH")  # isolating the character id_CH (i.e. removing the numbers typed as characters)
dim(char_only_aggregated <-aggregate(char_only$abundance, by= list(char_only$id_CH), sum))
# See the README files for notes about these
names(char_only_aggregated) = c("id_CH", "tot. abundance")
char_only_aggregated  # here are all 4 id's that pose a problem
phyto_raw <- phyto_raw %>% mutate(id_CH = replace(id_CH, id_CH == "Scenedesmus linearis", 3025)) # replaces the "Scenedesmus linearis" by its real id_CH. We chose to keep this taxon only
# Transform the id_CH to an Integer and remove the induced NA's
phyto_raw$id_CH <- as.integer(phyto_raw$id_CH)
phyto_raw <- phyto_raw[complete.cases(phyto_raw$id_CH),]  # The NA's removed had an id_CH written in Characters (see the step isolating id_CH that are species names)
phyto_raw$date <- as.Date(phyto_raw$date, "%d/%m/%Y")  # transforming the date to a Date format. Has to be done here, because it is a different format than in the other lakes
}
# Getting the phytoplankton data ready
ready_phyto_selec()  # setting the right data formats for the columns and adding the corresponding information from the meta database, for each entry
treat_noBiovolP(lake)  # Isolating the phyto data missing a biovolume value in the meta database
ready_phyto_clean()  # Removing entries without a metaDB-biovolume value and calculating the biovolume for the ones we keep
ready_phyto_agg()  # Aggregating phytoplankton biovolume per order and date
ready_phyto_final()  # Making the phytoplankton data ready for analysis and calculating difference and ratio between the phyto orders and the total phyto biovolume per day
# Getting the zooplankton data ready
ready_zoo_raw()  # Setting the right data formats
ready_zoo_agg()  # Aggregating the abundance of chosen orders/phyla/families per day
ready_zoo_final()  # Making the zooplankton data ready for analysis
# Getting the chemistry and temperature data ready
ready_temp_chem()  # Preparing the chemistry and temperature data
# Putting the phytoplankton, zooplankton and chemistry + temperature data into a single frame
ready_end_frame(lake)  # Gathering all the phytoplankton, zooplankton, temperature and chemistry data into a single data frame. And making it ready for analysis
# The next two lines makes the code run in parallel (in short: this means that the code runs faster). Not so important for the small size of the data sets I have, but it's a good habit to have (on my machine, it usually divides the modelling time by 3)
cl = makePSOCKcluster(5)
registerDoParallel(cl)
summary(end_frame)
end_frame <- end_frame %>% select(-c(date, depth))  #removing the date and the depth from the DF - we don't need those anymore
# log-transforming the response variables (common logarithm, i.e. base of 10)
end_frame <- end_frame %>% mutate(across(c(tot_phyto:tot_cyanobac, d_chrooco:d_cyanobac), ~log10(.)))  # log10-transformation
end_frame <- end_frame %>% mutate(across(c(tot_phyto:tot_cyanobac, d_chrooco:d_cyanobac), ~replace(., . == -Inf, 0)))  # log10(0) = -Inf, so we have to replace every -Inf with 0 after the log10-transformation
# 10.1) Random Forest on r_cyanobac ####
#'
#' Choosing r_cyanobac as the response variable and creating the models for this variable
#'
featurePlot(x = end_frame[, c("tot_phyto", "NH4_N", "NO3_N", "temperature")], y = end_frame$r_cyanobac, plot = "scatter", labels = c("", "ratio cyanobac - tot_phyto"))
# Selecting only the features we are interested in
r_cyanobac_frame <- end_frame %>% select(r_cyanobac, NH4_N:calanoida)  # for the total cyanobacteria ratio
chrooco_frame <- end_frame %>% select(r_chrooco, NH4_N:calanoida)  # The Chroococcales ratio
nosto_frame <- end_frame %>% select(r_nosto, NH4_N:calanoida)  # The Nostocales ratio
oscillato_frame <- end_frame %>% select(r_oscillato, NH4_N:calanoida)  # The Oscilaltoriales ratio
# 10.1.1) Random forest on the total cyanobacteria ratio
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(r_cyanobac_frame$r_cyanobac, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- r_cyanobac_frame[trainIndex, ]  # creating the training data set
test <- r_cyanobac_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(r_cyanobac_frame)
findCorrelation(frame_cor, verbose = TRUE)
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
# Running the actual Random Forest models
set.seed(123)
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))
# Displaying the result
rf_r_cyano
# Analyzing variable importance for the model (post-modelling)
varImp_r_cyano <- varImp(rf_r_cyano)
r_cyano_impPlot <- plot(varImp_r_cyano, main = "Variable importance (Random Forest, no imputation")
print(r_cyano_impPlot)
# 10.1.2) Random forest on the chroococcales ratio
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(chrooco_frame$r_chrooco, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- chrooco_frame[trainIndex, ]  # creating the training data set
test <- chrooco_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(chrooco_frame)
findCorrelation(frame_cor, verbose = TRUE)
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
# Running the actual Random Forest models
set.seed(123)
system.time(rf_chrooco <- train(r_chrooco ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))
# Displaying the result
rf_chrooco
# Analyzing variable importance for the model (post-modelling)
varImp_chrooco <- varImp(rf_chrooco)
chrooco_impPlot <- plot(varImp_chrooco, main = "Variable importance (Random Forest, no imputation")
print(chrooco_impPlot)
# 10.1.3) Random forest on the Nostocales ratio
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(nosto_frame$r_nosto, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- nosto_frame[trainIndex, ]  # creating the training data set
test <- nosto_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(nosto_frame)
findCorrelation(frame_cor, verbose = TRUE)
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
# Running the actual Random Forest models
set.seed(123)
system.time(rf_nosto <- train(r_nosto ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))
# Displaying the result
rf_nosto
# Analyzing variable importance for the model (post-modelling)
varImp_nosto <- varImp(rf_nosto)
nosto_impPlot <- plot(varImp_nosto, main = "Variable importance (Random Forest, no imputation")
print(nosto_impPlot)
# 10.1.4) Random forest on the Oscillatoriales ratio
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(oscillato_frame$r_oscillato, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- oscillato_frame[trainIndex, ]  # creating the training data set
test <- oscillato_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(oscillato_frame)
findCorrelation(frame_cor, verbose = TRUE)
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
# Running the actual Random Forest models
set.seed(123)
system.time(rf_oscillato <- train(r_oscillato ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))
# Displaying the result
rf_oscillato
# Analyzing variable importance for the model (post-modelling)
varImp_oscillato <- varImp(rf_oscillato)
oscillato_impPlot <- plot(varImp_oscillato, main = "Variable importance (Random Forest, no imputation")
print(oscillato_impPlot)
# 10.2) Results of the modelling, and 3D plots ####
# The results of the random forest models:
rf_r_cyano
rf_chrooco
rf_nosto
rf_oscillatio
# Creating the plots using the pdp library
library(pdp)
bwr <- colorRampPalette(c("blue", "white", "red"))
cyano_3d <- plotPartial(partial(rf_r_cyano, pred.var = c("PO4_P", "temperature")), contour = TRUE, col.regions = bwr, main = "tot cyano ratio")
chrooco_3d <- plotPartial(partial(rf_chrooco, pred.var = c("cyclopoida", "tot_zoo")), contour = TRUE, col.regions = bwr, main = "Chrooco ratio")
system.time(nosto_3d <- plotPartial(partial(rf_nosto, pred.var = c("temperature", "NO3_N")), contour = TRUE, col.regions = bwr, main = "Nosto ratio"))
oscillato_3d <- plotPartial(partial(rf_oscillato, pred.var = c("daphniidae", "cyclopoida")), contour = TRUE, col.regions = bwr, main = "Oscillato ratio")
all_3d <- ggarrange(cyano_3d, chrooco_3d,
nosto_3d, oscillato_3d,
nrow = 2, ncol= 2)
all_3d
# now with the temperature and PO4 only
cyano_3d_tp <- plotPartial(partial(rf_r_cyano, pred.var = c("PO4_P", "temperature")), contour = TRUE, col.regions = bwr, main = "tot cyano ratio")
chrooco_3d_tp <- plotPartial(partial(rf_chrooco, pred.var = c("PO4_P", "temperature")), contour = TRUE, col.regions = bwr, main = "Chrooco ratio")
system.time(nosto_3d_tp <- plotPartial(partial(rf_nosto, pred.var = c("PO4_P", "temperature")), contour = TRUE, col.regions = bwr, main = "Nosto ratio"))
oscillato_3d_tp <- plotPartial(partial(rf_oscillato, pred.var = c("PO4_P", "temperature")), contour = TRUE, col.regions = bwr, main = "Oscillato ratio")
all_3d_tp <- ggarrange(cyano_3d_tp, chrooco_3d_tp,
nosto_3d_tp, oscillato_3d_tp,
nrow = 2, ncol= 2)
all_3d_tp
end_frame %>% filter(is.na(.))
end_frame %>% filter(is.na(NH4_N))
nrow(end_frame %>% filter(is.na(NH4_N)))
nrow(end_frame %>% filter(is.na(PO4_P)))
nrow(end_frame %>% filter(is.na(NO3_N)))
end_frame %>% filter(is.na(NO3_N))
# 10.1) Random Forest on r_cyanobac ####
#'
#' Choosing r_cyanobac as the response variable and creating the models for this variable
#'
featurePlot(x = end_frame[, c("tot_phyto", "NH4_N", "NO3_N", "temperature")], y = end_frame$r_cyanobac, plot = "scatter", labels = c("", "ratio cyanobac - tot_phyto"))
# Selecting only the features we are interested in
r_cyanobac_frame <- end_frame %>% select(r_cyanobac, NH4_N:calanoida)  # for the total cyanobacteria ratio
chrooco_frame <- end_frame %>% select(r_chrooco, NH4_N:calanoida)  # The Chroococcales ratio
nosto_frame <- end_frame %>% select(r_nosto, NH4_N:calanoida)  # The Nostocales ratio
oscillato_frame <- end_frame %>% select(r_oscillato, NH4_N:calanoida)  # The Oscilaltoriales ratio
# 10.1.1) Random forest on the total cyanobacteria ratio
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(r_cyanobac_frame$r_cyanobac, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- r_cyanobac_frame[trainIndex, ]  # creating the training data set
test <- r_cyanobac_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(r_cyanobac_frame)
findCorrelation(frame_cor, verbose = TRUE)
# In the GRE data set, there is only 1 NA in the NO3_N predictor. Bag-imputing it
train_pre_imp <- preProcess(training, method = "bagImpute", verbose = TRUE)  # creating the models to impute the missing data
test_pre_imp <- preProcess(test, method = "bagImpute", verbose = TRUE)
training <- predict(train_pre_imp, newdata = training)  # actually imputing the missing data
test <- predict(test_pre_imp, newdata = test)
nrow(training %>% filter(is.na(NO3_N)))
nrow(test %>% filter(is.na(NO3_N)))
?trainControl
# Displaying the result
rf_r_cyano
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
fitControl <- trainControl(method = "cv", number = 10, repeats = 10)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 10)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "boot", number = 10)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "boot", number = 20)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "cv", number = 20)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "LOOCV", number = 10)  # set the cross-validation control to be the same for all models
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
n
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "LOOCV")  # set the cross-validation control to be the same for all models.
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models.
set.seed(123)  # Setting a seed, so that the model is reproducible
system.time(rf_r_cyano <- train(r_cyanobac ~., data = training, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl, na.action = na.omit))  # Running the actual Random Forest model
# Displaying the result
rf_r_cyano
?train
