install.packages(c("cli", "lubridate", "RcppEigen", "stringi", "timechange"))
install.packages("lubridate")
R.Version()
print(R.Version())
v <- R.Version()
print(v)
install.packages("installr")
library(installr)
updateR()
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
source("A0_functions.R")  # The functions needed to run the code are stored in a separate file. This command imports the functions.
# Loading the necessary libraries
load_libraries()
# loading the data files
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new")
databases()  # loading the databases metaDBphyto and metaDBzoo
phyto_raw <- read.csv("phyto.smallzoo.86-19_BAL.csv", sep = ",")  # loading the raw phytoplankton data
zoo_raw <- read.csv("largezoo.86-19_BAL.csv")  # loading the raw zooplankton data
setwd("~/GitHub/Bachelor_cyanobac/workplace2/data_new/temp_chem")  # loading the raw temperature and chemistry data
temp_chem <- read.csv("temp.chem.85-20_BAL.csv")
lake <- "BAL"  # Defining the name of the lake. This will be used when saving data in external files, to identify them later on
# Getting the phytoplankton data ready
ready_phyto_selec()  # setting the right data formats for the columns and adding the corresponding information from the meta database, for each entry
treat_noBiovolP(lake)  # Isolating the phyto data missing a biovolume value in the meta database
ready_phyto_clean()  # Removing entries without a metaDB-biovolume value and calculating the biovolume for the ones we keep
ready_phyto_agg()  # Aggregating phytoplankton biovolume per order and date
ready_phyto_final()  # Making the phytoplankton data ready for analysis and calculating difference and ratio between the phyto orders and the total phyto biovolume per day
# Getting the zooplankton data ready
ready_zoo_raw()  # Setting the right data formats
ready_zoo_agg()  # Aggregating the abundance of chosen orders/phyla/families per day
ready_zoo_final()  # Making the zooplankton data ready for analysis
# Getting the chemistry and temperature data ready
ready_temp_chem()  # Preparing the chemistry and temperature data
# It is best not to make the plots in functions, since the axes have to be set manually for each lake
{
# temp_chem %>% filter(NO3_N > 10)  # The line to show the outlier
temp_chem <- temp_chem %>% filter(NO3_N < 10)  # there is an outlier that's falsifying the whole data (and it isn't realistic) -> this step excludes it
# Creating the plots with the evolution of the temperature and chemistry of the lake over time
# Plotting the temperature alone
temp <- ggplot(data = temp_chem, aes(x = date, y = temperature)) +
geom_point(shape = 20, colour = "aquamarine4", size = 0.5) +
geom_smooth(colour = "aquamarine4") +
ggtitle(label = "Temperature over the years") +
labs(x = "Date", y = "Temperature [°C]")
# Plotting the Phosphate alone
po4 <- ggplot(data = temp_chem, aes(x = date, y = PO4_P)) +
geom_point(shape = 20, colour = "firebrick3", size = 0.5) +
geom_smooth(colour = "firebrick3") +
ggtitle(label = "PO4 over the years") +
labs(x = "Date", y = "PO4-P concentration [mg/L]")
# Plotting the Ammonia alone
nh4 <- ggplot(data = temp_chem, aes(x = date, y = NH4_N)) +
geom_point(shape = 20, colour = "cornflowerblue", size = 0.5) +
geom_smooth() +
ggtitle(label = "NH4 over the years") +
labs(x = "Date", y = "NH4-N concentration [mg/L]") +
scale_y_continuous(lim = c(0, 0.15))
# Plotting the Nitrate alone
no3 <- ggplot(data = temp_chem, aes(x = date, y = NO3_N)) +
geom_point(shape = 20, colour = "darkgoldenrod1", size = 0.5) +
geom_smooth(colour = "darkgoldenrod1") + # adding 'method = "gam"' shows the oscillation of the data points even more
ggtitle(label = "NO3 over the years") +
labs(x = "Date", y = "NO3-N concentration [mg/L]")
figure_tempchem <- ggarrange(temp, po4,
nh4, no3,
ncol = 2, nrow = 2)  # Putting the plots about chemistry and temperature together into a figure
figure_tempchem <- annotate_figure(figure_tempchem, top = text_grob(paste(lake, "lake"), face = "bold", size = 20))  # Adding a title to the figure
# Plotting the Nitrate and Ammonia in a single plot
coeff1 <- 20  # This variable is used to change the right y-axis size
no3_nh4 <- ggplot(data = temp_chem) +
geom_point(aes(x = date, y = NH4_N * coeff1, colour = "NH4"), size = 0.5) +
geom_point(aes(x = date, y = NO3_N, colour = "NO3"), size = 0.5) +
geom_smooth(aes(x = date, y = NH4_N * coeff1, colour = "NH4")) +
geom_smooth(aes(x = date, y = NO3_N, colour = "NO3")) +
scale_y_continuous(name = "NO3-N [mg/L]", lim = c(0, 3.5), sec.axis = sec_axis(trans=~./coeff1, name = "NH4-N [mg/L]")) +
ggtitle(label = "NO3 and NH4 over the years") +
scale_color_manual(name = "", breaks = c("NO3", "NH4"), values = c("NO3" = "darkgoldenrod1", "NH4" = "cornflowerblue")) +
theme(legend.position = "bottom")
# Plotting the Phosphate and temperature in a single plot
coeff2 <- 60
temp_po4 <- ggplot(data = temp_chem) +
geom_point(aes(x = date, y = temperature, colour = "Temperature"), size = 0.5) +
geom_point(aes(x = date, y = PO4_P * coeff2, colour = "PO4"), size = 0.5) +
geom_smooth(aes(x = date, y = temperature, colour = "Temperature")) +
geom_smooth(aes(x = date, y = PO4_P * coeff2, colour = "PO4")) +
scale_y_continuous(name = "Temperature [°C]", sec.axis = sec_axis(trans=~./coeff2, name = "PO4-P [mg/L]")) +
ggtitle(label = "Temperature and PO4 over the years") +
scale_color_manual(name = "", breaks = c("Temperature", "PO4"), values = c("Temperature" = "aquamarine4", "PO4" = "firebrick3")) +
theme(legend.position = "bottom", plot.title = element_text(size = 10))
} #
# Putting the phytoplankton, zooplankton and chemistry + temperature data into a single frame
ready_end_frame()  # Gathering all the phytoplankton, zooplankton, temperature and chemistry data into a single data frame. And making it ready for analysis
summary(end_frame)
end_frame <- end_frame %>% select(-c(date, depth))  #removing the date and the depth from the DF - we don't need those anymore
# Working with the r_cyanobac as a response variable
# Would make sense to create a function creating the 3D graphs he showed me
featurePlot(x = end_frame[, c("tot_phyto", "NH4_N", "NO3_N", "temperature")], y = end_frame$r_cyanobac, plot = "scatter", labels = c("", "ratio cyanobac - tot_phyto"))
r_cyanobac_frame <- end_frame %>% select(r_cyanobac, NH4_N:calanoida)
# Partitioning the data
set.seed(123) # doesn't need to be 123, but having a value here makes the test reproducible
trainIndex <- createDataPartition(r_cyanobac_frame$r_cyanobac, p = 0.7, list = FALSE)  # Choosing which indices of the data set will be for training, and which ones for testing. 70% are for training, 30% for testing
training <- r_cyanobac_frame[trainIndex, ]  # creating the training data set
test <- r_cyanobac_frame[-trainIndex, ]  # creating the testing data set
# Finding out if there's correlated variables in the data set
frame_cor <- cor(r_cyanobac_frame)
findCorrelation(frame_cor, verbose = TRUE)
# Realizing the imputation:
# For the training set
pre_knn <- preProcess(training, method = "knnImpute", k = round(sqrt(nrow(training))), verbose = T)
pre_bag <- preProcess(training, method = "bagImpute", verbose = T)
pre_median <- preProcess(training, method = "medianImpute", verbose = T)
#for the testing set:
test_pre_knn <- preProcess(test, method = "knnImpute", k = round(sqrt(nrow(training))), verbose = T)
test_pre_bag <- preProcess(test, method = "bagImpute", verbose = T)
test_pre_median <- preProcess(test, method = "medianImpute", verbose = T)
# Applying it to the data:
# For the training set
imputed_knn <- predict(pre_knn, newdata = training)  # Warning: this automatically centers and scales the data. Means that following formula is applied to it: (X - mean(column) / sd(column))
imputed_bag <- predict(pre_bag, newdata = training)
imputed_median <- predict(pre_median, newdata = training)
# For the testing set
test_imputed_knn <- predict(test_pre_knn, newdata = test)  # Warning: this automatically centers and scales the data. Means that following formula is applied to it: (X - mean(column) / sd(column))
test_imputed_bag <- predict(test_pre_bag, newdata = test)
test_imputed_median <- predict(test_pre_median, newdata = test)
# Evaluating algorithmically which variables contain low information, and that could thus be removed (pre-modelling):
subsets <- c(1:9)  # specifying the amount of variables I want to have in each test (here, the computed will be conducting tests with all the possible amounts of variables)
# for the kNN-imputed data:
rfeCtrl_knn <- rfeControl(functions = rfFuncs, method = "cv", verbose = FALSE)  # the first arguments allows to specify which method we want to use
system.time(rfProfile_knn <- rfe(x = imputed_knn[, -1], y = imputed_knn$r_cyanobac, sizes = subsets, rfeControl = rfeCtrl_knn))  # this code always takes a bit of time, which is totally normal
# for the bag-imputed data:
rfeCtrl_bag <- rfeControl(functions = rfFuncs, method = "cv", verbose = FALSE)  # the first arguments allows to specify which method we want to use
system.time(rfProfile_bag <- rfe(x = imputed_bag[, -1], y = imputed_bag$r_cyanobac, sizes = subsets, rfeControl = rfeCtrl_bag))  # this code always takes a bit of time, which is totally normal
# for the median-imputed data:
rfeCtrl_median <- rfeControl(functions = rfFuncs, method = "cv", verbose = FALSE)  # the first arguments allows to specify which method we want to use
system.time(rfProfile_median <- rfe(x = imputed_median[, -1], y = imputed_median$r_cyanobac, sizes = subsets, rfeControl = rfeCtrl_median))  # this code always takes a bit of time, which is totally normal
# Displaying the results:
rfProfile_knn
rfProfile_bag
rfProfile_median
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_median <- train(r_cyanobac ~., data = imputed_median, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
# Displaying their results
rf_knn
rf_bag
rf_median
tic <- Sys.time()
cl = makePSOCKcluster(5)  # specifying how many versions of R we want to have running simultaneously
registerDoParallel(cl)  # start the cluster, i.e. get 5 versions of R running in parallel
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_median <- train(r_cyanobac ~., data = imputed_median, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
stopCluster(cl)  # stopping the cluster
toc <- Sys.time()
print(toc - tic)
tic <- Sys.time()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
?summary.connection
setwd("~/GitHub/Bachelor_cyanobac/workplace2/Code")
tic <- Sys.time()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
?Error
?error
error
Error
?error()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
registerDoParallel(cl)  # start the cluster, i.e. get 5 versions of R running in parallel
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
stopCluster(cl)  # stopping the cluster
# Running the actual Random Forest models, in parallel (to save time. Not particularly important here, but can get crucial if using bigger data sets)
tic <- Sys.time()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
cl = makePSOCKcluster(5)  # specifying how many versions of R we want to have running simultaneously
registerDoParallel(cl)  # start the cluster, i.e. get 5 versions of R running in parallel
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_median <- train(r_cyanobac ~., data = imputed_median, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
stopCluster(cl)  # stopping the cluster
stopImplicitCluster()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
# Running the actual Random Forest models, in parallel (to save time. Not particularly important here, but can get crucial if using bigger data sets)
tic <- Sys.time()
cl = makePSOCKcluster(5)  # specifying how many versions of R we want to have running simultaneously
registerDoParallel(cl, cores = 4)  # start the cluster, i.e. get 5 versions of R running in parallel
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
rf_median <- train(r_cyanobac ~., data = imputed_median, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
stopCluster(cl)  # stopping the cluster
toc <- Sys.time()
print(toc - tic)  # this line prints the time difference between the beginning and the end of the modellization done by the computer
tic <- Sys.time()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
toc <- Sys.time()
print(toc - tic)  # this line prints the time difference between the beginning and the end of the modellization done by the computer
cl = makePSOCKcluster(5)  # specifying how many versions of R we want to have running simultaneously
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
View(noBiovolP)
View(cl)
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
stopCluster(cl)  # stopping the cluster
rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
?makePSOCKcluster
setDefaultCluster(cl = NULL)
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
getDefaultCluster()
cl = makePSOCKcluster(1)  # specifying how many versions of R we want to have running simultaneously
getDefaultCluster()
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
cl = makePSOCKcluster(1)  # specifying how many versions of R we want to have running simultaneously
registerDoParallel(cl, cores = 4)  # start the cluster, i.e. get 5 versions of R running in parallel
rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl)
system.time(rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl))
stopCluster(cl)
tg <- data.frame(mtry = c(1:9))  # set the tuneGrid argument to be the same for all models
fitControl <- trainControl(method = "cv", number = 10)  # set the cross-validation control to be the same for all models
system.time(rf_knn <- train(r_cyanobac ~., data = imputed_knn, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl))
system.time(rf_bag <- train(r_cyanobac ~., data = imputed_bag, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl))
system.time(rf_median <- train(r_cyanobac ~., data = imputed_median, method = "rf", importance = TRUE, tuneGrid = tg, trControl = fitControl))
